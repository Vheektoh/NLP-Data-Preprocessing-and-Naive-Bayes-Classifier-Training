{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa450f8",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer using NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c96f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28caf71",
   "metadata": {},
   "source": [
    "#### configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ab6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'eL2DeeOX1wX7HH0Ldu3Jy6qNT'\n",
    "api_key_secret = '0QnVbYfZlYxzuJI4Mx9GDIEb8uiQJlLoMBCyDH4V1KvxeCt6tF'\n",
    "access_token = '852228407140261893-0sCUBrO7a2lnroq6sFhcRqCnn3QdPsS'\n",
    "access_token_secret = 'qcHOxjOfFmrBaTkNp0rFJLLCtfL0hSR2ytQGOWlUo05Zj'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218f936",
   "metadata": {},
   "source": [
    "#### authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5952725",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a543da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: tweets_mode\n"
     ]
    }
   ],
   "source": [
    "Elon_musk_tweet = api.user_timeline(user_id = \"Elon Musk\", screen_name = \"elonmusk\", tweets_mode = \"extended\", include_rts = False, count = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e5b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for tweet in Elon_musk_tweet:\n",
    "    data.append([tweet.created_at, tweet.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f322b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Time', 'Tweet']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7364e",
   "metadata": {},
   "source": [
    "#### Import all csv files for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7c7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tesla_data = pd.read_csv('C:/Users/personal/Documents/datasets/TSLA.csv')\n",
    "Tweets = pd.read_csv('tweets.csv')\n",
    "raw_text = Tweets[\"Tweet\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90722ce",
   "metadata": {},
   "source": [
    "#### convert raw text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = [] \n",
    "def to_lower_case(data):\n",
    "    for words in raw_text:\n",
    "        lower_text.append(str.lower(words))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea13739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_lower_case(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45952bf",
   "metadata": {},
   "source": [
    "#### tokenize list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211d2ad",
   "metadata": {},
   "source": [
    "I will perform an overview of what basic processes of NLP sentence analyzer is all about below before diving fully into \n",
    "the main processes in the second half of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0857e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee83baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_toke = []\n",
    "for e in lower_text:\n",
    "    e = sent_tokenize(e)\n",
    "    sent_toke.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081dec62",
   "metadata": {},
   "source": [
    "#### word tokenize list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ffcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f105f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = [word_tokenize(i) for i in lower_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09778c",
   "metadata": {},
   "source": [
    "#### cleaning special characters using regex(regular expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf6a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_3 = []\n",
    "for words in clean_text:\n",
    "    clean = []\n",
    "    for w in words:\n",
    "        regex = re.sub(r'[^\\w\\s]', \" \", w)\n",
    "        if regex != \" \":\n",
    "            clean.append(regex)\n",
    "        clean_text_3.append(clean)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad839d2",
   "metadata": {},
   "source": [
    "## NOTE:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2aacbc",
   "metadata": {},
   "source": [
    "From the above processes, the tweet column from the dataframe was turned into a list, making it almost tasking to check\n",
    "the relationship with the date column from the dataframe also...and would still pose another difficulty if we want to compare\n",
    "it with the date from the Stock data and do some meaningful analysis from it.\n",
    "So, owing to this problem, i will perfom another set of Tokenisation on the column itself using the dataframe!\n",
    "I will start the tokenisation directly from the tweets.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b82eb",
   "metadata": {},
   "source": [
    "#### Tokenising the tweet from the tweet df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70acc97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-30 00:23:46+00:00</td>\n",
       "      <td>@HistoryInPics Great style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-30 00:19:27+00:00</td>\n",
       "      <td>@TeslaPodcast @SenateDems @Sen_JoeManchin @Sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-30 00:09:07+00:00</td>\n",
       "      <td>Heatwave in Shortville 🥵</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-29 23:04:22+00:00</td>\n",
       "      <td>@ICannot_Enough @DirtyTesLa 10.13 is probably ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-29 15:12:58+00:00</td>\n",
       "      <td>@DirtyTesLa We’re working super hard on 10.13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>189</td>\n",
       "      <td>2022-07-12 04:55:53+00:00</td>\n",
       "      <td>@alexkehr @BillyM2k @levie I do recommend a lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>190</td>\n",
       "      <td>2022-07-12 04:39:44+00:00</td>\n",
       "      <td>@levie And too many little red or green LED in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>191</td>\n",
       "      <td>2022-07-12 04:38:33+00:00</td>\n",
       "      <td>@BillyM2k @levie YouTube keeps playing me the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>192</td>\n",
       "      <td>2022-07-12 04:33:59+00:00</td>\n",
       "      <td>@EvaFoxU 🤣</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>193</td>\n",
       "      <td>2022-07-12 04:29:56+00:00</td>\n",
       "      <td>@PPathole @planet4589 @NASASpaceflight That is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                       Time  \\\n",
       "0             0  2022-07-30 00:23:46+00:00   \n",
       "1             1  2022-07-30 00:19:27+00:00   \n",
       "2             2  2022-07-30 00:09:07+00:00   \n",
       "3             3  2022-07-29 23:04:22+00:00   \n",
       "4             4  2022-07-29 15:12:58+00:00   \n",
       "..          ...                        ...   \n",
       "189         189  2022-07-12 04:55:53+00:00   \n",
       "190         190  2022-07-12 04:39:44+00:00   \n",
       "191         191  2022-07-12 04:38:33+00:00   \n",
       "192         192  2022-07-12 04:33:59+00:00   \n",
       "193         193  2022-07-12 04:29:56+00:00   \n",
       "\n",
       "                                                 Tweet  \n",
       "0                           @HistoryInPics Great style  \n",
       "1    @TeslaPodcast @SenateDems @Sen_JoeManchin @Sen...  \n",
       "2                             Heatwave in Shortville 🥵  \n",
       "3    @ICannot_Enough @DirtyTesLa 10.13 is probably ...  \n",
       "4    @DirtyTesLa We’re working super hard on 10.13,...  \n",
       "..                                                 ...  \n",
       "189  @alexkehr @BillyM2k @levie I do recommend a lo...  \n",
       "190  @levie And too many little red or green LED in...  \n",
       "191  @BillyM2k @levie YouTube keeps playing me the ...  \n",
       "192                                         @EvaFoxU 🤣  \n",
       "193  @PPathole @planet4589 @NASASpaceflight That is...  \n",
       "\n",
       "[194 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13167062",
   "metadata": {},
   "source": [
    "Iterate over the column to remove punctuation using the regex and apply function and then the \n",
    "stopwords respectively before going to the next step which would be tokenisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1edeede",
   "metadata": {},
   "source": [
    "## What is Regex?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec2512",
   "metadata": {},
   "source": [
    "A Regular Expression (or Regex) is a pattern (or filter) that describes a set of strings that matches the pattern. \n",
    "In other words, a regex accepts a certain set of strings and rejects the rest.\n",
    "Regular expressions are particularly useful for defining filters.\n",
    "Regular expressions contain a series of characters that define a pattern of text to be matched—to make a filter more specialized, or general.\n",
    "For example, the regular expression ^AL[.]* searches for all items beginning with AL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9640d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyper_strings(tweet):\n",
    "    clean_tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "    clean_tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet)\n",
    "\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3003c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[\"Tweet\"] = Tweets[\"Tweet\"].apply(remove_hyper_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec84bd",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fede2",
   "metadata": {},
   "source": [
    "When we deal with text, we need to break it down into smaller pieces for analysis. To \n",
    "do this, tokenization can be applied. Tokenization is the process of dividing text into \n",
    "a set of pieces, such as words or sentences. These pieces are called tokens. Depending \n",
    "on what we want to do, we can define our own methods to divide the text into many \n",
    "tokens. Let's look at how to tokenize the input text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f1b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    \n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    return  tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6746d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[\"Tweet\"] = Tweets[\"Tweet\"].apply(tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4f6b1",
   "metadata": {},
   "source": [
    "## What are Stopwords?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9be5a",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in any language.\n",
    "For example, in English, “the”, “is” and “and”, would easily qualify as stop words.\n",
    "In NLP and text mining applications, stop words are used to eliminate unimportant words,\n",
    "allowing applications to focus on the important words instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c56946",
   "metadata": {},
   "source": [
    "#### removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee252b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stopwords_english = stopwords.words('english')\n",
    "\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def remove_stopwords_punctuations(tweet_tokens):\n",
    "    \n",
    "    clean_tweets = []\n",
    "    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and word not in punctuation):\n",
    "            clean_tweets.append(word)\n",
    "            \n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df17c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[\"Tweet\"] = Tweets[\"Tweet\"].apply(remove_stopwords_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb421072",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae11a5",
   "metadata": {},
   "source": [
    "Stemming is a technique used to extract the base form of the words by removing affixes from them.\n",
    "It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
    "Search engines use stemming for indexing the words.\n",
    "It is the process of converting words to its most general form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "685e2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def get_stem(tweets_clean):\n",
    "    \n",
    "    stem_tweets = []\n",
    "    \n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        stem_tweets.append(stem_word)\n",
    "        \n",
    "    return stem_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7245a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[\"Tweet\"] = Tweets[\"Tweet\"].apply(get_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9407c",
   "metadata": {},
   "source": [
    "## Lemmatiztion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b1608",
   "metadata": {},
   "source": [
    "What is meant by lemmatization?\n",
    "Lemmatization is the grouping together of different forms of the same word.\n",
    "In search queries, lemmatization allows end users to query any version of a base word and get relevant results.\n",
    "In Lemmatization root word is called Lemma.\n",
    "A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972b0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatize = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tweet(tweet_lem):\n",
    "    \n",
    "    final_tweet = []\n",
    "    \n",
    "    for word in tweet_lem:\n",
    "        lem_word = lemmatize.lemmatize(word)\n",
    "        final_tweet.append(lem_word)\n",
    "        \n",
    "    return final_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0821442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[\"Tweet\"] = Tweets[\"Tweet\"].apply(lemmatize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a514c",
   "metadata": {},
   "source": [
    "# Combine Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd025dac",
   "metadata": {},
   "source": [
    "Putting the above methods in a sequential method to ease future work when working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1595bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    processed_tweet = remove_hyper_strings(tweet)\n",
    "    tweet_tokens = tokenize_tweet(processed_tweet)\n",
    "    clean_tweets = remove_stopwords_punctuations(tweet_tokens)\n",
    "    stem_tweets = get_stem(clean_tweets)\n",
    "    final_tweet = lemmatize_tweet(stem_tweets)\n",
    "    \n",
    "    return final_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "548ed85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_example = 'you mean you' \n",
    "\n",
    "project_test = process_tweet(tweet_example)\n",
    "\n",
    "project_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3af8c0",
   "metadata": {},
   "source": [
    "## Building a Sentiment Analyzer using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c89a8",
   "metadata": {},
   "source": [
    "Sentiment analysis is the process of determining the sentiment of a piece of text. \n",
    "For example, it can be used to determine whether a movie review is positive \n",
    "or negative.\n",
    "We will use a Naive Bayes classifier to build this sentiment analyzer. First, extract all \n",
    "the unique words from the text. The NLTK classifier needs this data to be arranged \n",
    "in the form of a dictionary so that it can ingest it. Once the text data is divided into \n",
    "training and testing datasets, the Naive Bayes classifier will be trained to classify \n",
    "the reviews into positive and negative. Afterward, the top most informative words \n",
    "to indicate positive and negative reviews can be calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3c6d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03365f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(words):\n",
    "     return dict([(word, True) for word in words])\n",
    "    \n",
    "def load_file():\n",
    "    # Load the reviews from the corpus \n",
    "    fileids_pos = movie_reviews.fileids('pos')\n",
    "    fileids_neg = movie_reviews.fileids('neg')\n",
    "    \n",
    "    # Extract the features from the reviews\n",
    "    features_pos = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in fileids_pos]\n",
    "    features_neg = [(extract_features(movie_reviews.words(fileids=[f])), 'Negative') for f in fileids_neg]\n",
    "    \n",
    "    return(features_pos, features_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba87708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pos, features_neg = load_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f68507",
   "metadata": {},
   "source": [
    "#### Define the split between training and testing. In this case, we will allocate 80% for training and 20% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b832de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "num_pos = int(threshold * len(features_pos))\n",
    "num_neg = int(threshold * len(features_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1216994",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = features_pos[:num_pos] + features_neg[:num_neg]\n",
    "features_test = features_pos[num_pos:] + features_neg[num_neg:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d8ebe5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b0d12b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d3e97",
   "metadata": {},
   "source": [
    "Train a NaiveBayesClassifier using the training data and compute the accuracy \n",
    "using the inbuilt accuracy method available in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62984e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayesClassifier.train(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614886f",
   "metadata": {},
   "source": [
    "Check for model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a71d9381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_acc(clf, features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61b85093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9825"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_acc(clf, features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2da5c9",
   "metadata": {},
   "source": [
    "# Test Model on Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17c60e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get dataframe column as a series\n",
    "list_of_tweet = Tweets[\"Tweet\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57746d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in list_of_tweet:\n",
    "    review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f61703e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Probability\n",
    "probabilities = clf.prob_classify(extract_features(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a970c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = probabilities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa310fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6625796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(probabilities.prob(predicted_sentiment), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d37b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
